{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "from enum import Enum\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "from random import randrange\n",
    "import random\n",
    "import math\n",
    "import pypianoroll\n",
    "from utils.midi_utils import print_midi_info, play_midi, plot_pianoroll, get_music_metrics, process_pianoroll, process_midi\n",
    "from constants import Constants\n",
    "from augmentation import AddAndRemoveAPercentageOfNotes\n",
    "from data_generator import PianoRollGenerator\n",
    "from utils.generate_training_plots import GenerateTrainingPlots\n",
    "from inference import Inference\n",
    "from model import OptimizerType\n",
    "from model import ArCnnModel\n",
    "\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_lower_bound_remove = 0 \n",
    "sampling_upper_bound_remove = 100\n",
    "sampling_lower_bound_add = 1\n",
    "sampling_upper_bound_add = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized loss function\n",
    "class Loss():\n",
    "    @staticmethod \n",
    "    def built_in_softmax_kl_loss(target, output):\n",
    "        '''\n",
    "        Custom Loss Function\n",
    "        :param target: ground truth values\n",
    "        :param output: predicted values\n",
    "        :return kullback_leibler_divergence loss\n",
    "        '''\n",
    "        target = K.flatten(target)\n",
    "        output = K.flatten(output)\n",
    "        target = target / K.sum(target)\n",
    "        output = K.softmax(output)\n",
    "        return keras.losses.kullback_leibler_divergence(target, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the MIDI files from the data_dir and save them with the midi_files variable  \n",
    "midi_files = []\n",
    "#midi_files.extend(glob.glob('data/gb/**/*.mid'))\n",
    "#midi_files.extend(glob.glob('data/gb/tetris/*.mid'))\n",
    "midi_files.extend(glob.glob('data/ms/**/*.mid'))\n",
    "#midi_files.extend(glob.glob('data/master_system/*.mid'))\n",
    "#midi_files.extend(glob.glob('data/nes/**/*.mid'))\n",
    "#midi_files.extend(glob.glob('data/snes/**/*.mid'))\n",
    "\n",
    "all_programs_used = []\n",
    "all_voices_used = []\n",
    "all_drums_used = []\n",
    "\n",
    "# Generate MIDI file samples\n",
    "def generate_samples(midi_files, bars, beats_per_bar, beat_resolution, bars_shifted_per_sample):\n",
    "    \"\"\"\n",
    "    dataset_files: All files in the dataset\n",
    "    return: piano roll samples sized to X bars\n",
    "    \"\"\"\n",
    "    timesteps_per_nbars = bars * beats_per_bar * beat_resolution\n",
    "    time_steps_shifted_per_sample = bars_shifted_per_sample * beats_per_bar * beat_resolution\n",
    "    samples = []\n",
    "    for midi_file in midi_files:\n",
    "        print('process ' + midi_file + '...')\n",
    "        print_midi_info(midi_file,\n",
    "                        all_programs_used=all_programs_used, \n",
    "                        all_voices_used=all_voices_used, \n",
    "                        all_drums_used=all_drums_used)\n",
    "        pianoroll, drums = process_midi(midi_file, beat_resolution, Constants.program) # Parse the MIDI file and get the piano roll\n",
    "        samples.extend(process_pianoroll(pianoroll, drums, time_steps_shifted_per_sample, timesteps_per_nbars))\n",
    "    \n",
    "    print('all programs used: ' + str(np.unique(all_programs_used, return_counts=True)))\n",
    "    print('all voices used (min/max): {}/{}'.format(\n",
    "        min(np.unique(all_voices_used)),\n",
    "        max(np.unique(all_voices_used))))\n",
    "    print('all drums used: ' + str(np.unique(all_drums_used, return_counts=True)))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Saving the generated samples into a dataset variable \n",
    "dataset_samples = generate_samples(midi_files, Constants.bars, Constants.beats_per_bar,Constants.beat_resolution, Constants.bars_shifted_per_sample)\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(dataset_samples);\n",
    "\n",
    "dataset_size = len(dataset_samples)\n",
    "dataset_split = math.floor(dataset_size * Constants.training_validation_split) \n",
    "\n",
    "training_samples = dataset_samples[0:dataset_split]\n",
    "print(\"training samples length: {}\".format(len(training_samples)))\n",
    "validation_samples = dataset_samples[dataset_split + 1:dataset_size]\n",
    "print(\"validation samples length: {}\".format(len(validation_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piano Roll Input Dimensions\n",
    "input_dim = (Constants.bars * Constants.beats_per_bar * Constants.beat_resolution, \n",
    "             Constants.number_of_pitches, \n",
    "             Constants.number_of_channels)\n",
    "# Number of Filters In The Convolution\n",
    "num_filters = 32\n",
    "# Growth Rate Of Number Of Filters At Each Convolution\n",
    "growth_factor = 2\n",
    "# Number Of Encoder And Decoder Layers\n",
    "num_layers = 5\n",
    "# A List Of Dropout Values At Each Encoder Layer\n",
    "dropout_rate_encoder = [0, 0.5, 0.5, 0.5, 0.5]\n",
    "# A List Of Dropout Values At Each Decoder Layer\n",
    "dropout_rate_decoder = [0.5, 0.5, 0.5, 0.5, 0]\n",
    "# A List Of Flags To Ensure If batch_normalization Should be performed At Each Encoder\n",
    "batch_norm_encoder = [True, True, True, True, False]\n",
    "# A List Of Flags To Ensure If batch_normalization Should be performed At Each Decoder\n",
    "batch_norm_decoder = [True, True, True, True, False]\n",
    "# Path to Pretrained Model If You Want To Initialize Weights Of The Network With The Pretrained Model\n",
    "pre_trained = False\n",
    "# Learning Rate Of The Model\n",
    "learning_rate = 0.001\n",
    "# Optimizer To Use While Training The Model\n",
    "optimizer_enum = OptimizerType.ADAM\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "# Number Of Epochs\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Number of Batch Iterations Before A Training Epoch Is Considered Finished\n",
    "steps_per_epoch = int(\n",
    "    len(training_samples) * Constants.samples_per_ground_truth_data_item / int(batch_size))\n",
    "\n",
    "print(\"The Total Number Of Steps Per Epoch Are: \"+ str(steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Data Generator\n",
    "training_data_generator = PianoRollGenerator(sample_list=training_samples,\n",
    "                                             sampling_lower_bound_remove = sampling_lower_bound_remove,\n",
    "                                             sampling_upper_bound_remove = sampling_upper_bound_remove,\n",
    "                                             sampling_lower_bound_add = sampling_lower_bound_add,\n",
    "                                             sampling_upper_bound_add = sampling_upper_bound_add,\n",
    "                                             batch_size = batch_size,\n",
    "                                             bars = Constants.bars,\n",
    "                                             samples_per_data_item = Constants.samples_per_ground_truth_data_item,\n",
    "                                             beat_resolution = Constants.beat_resolution,\n",
    "                                             beats_per_bar = Constants.beats_per_bar,\n",
    "                                             number_of_pitches = Constants.number_of_pitches,\n",
    "                                             number_of_channels = Constants.number_of_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data Generator\n",
    "validation_data_generator = PianoRollGenerator(sample_list = validation_samples,\n",
    "                                               sampling_lower_bound_remove = sampling_lower_bound_remove,\n",
    "                                               sampling_upper_bound_remove = sampling_upper_bound_remove,\n",
    "                                               sampling_lower_bound_add = sampling_lower_bound_add,\n",
    "                                               sampling_upper_bound_add = sampling_upper_bound_add,\n",
    "                                               batch_size = batch_size, \n",
    "                                               bars = Constants.bars,\n",
    "                                               samples_per_data_item = Constants.samples_per_ground_truth_data_item,\n",
    "                                               beat_resolution = Constants.beat_resolution,\n",
    "                                               beats_per_bar = Constants.beats_per_bar, \n",
    "                                               number_of_pitches = Constants.number_of_pitches,\n",
    "                                               number_of_channels = Constants.number_of_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback For Loss Plots \n",
    "plot_losses = GenerateTrainingPlots()\n",
    "## Checkpoint Path\n",
    "checkpoint_filepath =  'best-model-new.hdf5'\n",
    "\n",
    "# Callback For Saving Model Checkpoints \n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Callback for logging\n",
    "csv_logger = keras.callbacks.CSVLogger('training_log_drums2.csv', append=True, separator=',')\n",
    "\n",
    "# Create A List Of Callbacks\n",
    "callbacks_list = [plot_losses, model_checkpoint_callback, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A Model Instance\n",
    "MusicModel = ArCnnModel(input_dim = input_dim,\n",
    "                        num_filters = num_filters,\n",
    "                        growth_factor = growth_factor,\n",
    "                        num_layers = num_layers,\n",
    "                        dropout_rate_encoder = dropout_rate_encoder,\n",
    "                        dropout_rate_decoder = dropout_rate_decoder,\n",
    "                        batch_norm_encoder = batch_norm_encoder,\n",
    "                        batch_norm_decoder = batch_norm_decoder,\n",
    "                        pre_trained = pre_trained,\n",
    "                        learning_rate = learning_rate,\n",
    "                        optimizer_enum = optimizer_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicModel.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from previous training instead if the file already exists\n",
    "old_checkpoint_filepath =  'best-model-new.hdf5'\n",
    "if os.path.isfile(old_checkpoint_filepath):\n",
    "    model = load_model(old_checkpoint_filepath, \n",
    "                       custom_objects={'built_in_softmax_kl_loss': Loss.built_in_softmax_kl_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "history = model.fit_generator(training_data_generator,\n",
    "                              validation_data = validation_data_generator,\n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              epochs = epochs,\n",
    "                              callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
